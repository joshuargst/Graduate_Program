---
title: "DATA 606 Spring 2020 - Final Exam"
author: "Joshua Registe"
output:
  pdf_document: default
  html_document:
    df_print: paged
---

```{r, eval=FALSE, echo=FALSE}
# The following two commands will install a LaTeX installation so that the document can be compiled to PDF. These only need to be run once per R installation.
install.packages(c('tinytex','kableExtra'))

tinytex::install_tinytex()
```

```{r, echo=FALSE}
options(digits = 2)
library(dplyr)

```

# Part I

Please put the answers for Part I next to the question number (2pts each):

1.  b
2.  a
3.  a
4.  d 
5.  b
6.  d

7a. Describe the two distributions (2pts).

The first distribution is a right skewed distribution

The second distribution is normal and follows a gaussian curve

7b. Explain why the means of these two distributions are similar but the standard deviations are not (2 pts).

The means are similar because most of the points lie within similar ranges, on observation A, most of the points lie between 4 and 6 while on B, most of the points lie between 4.5 and 5.5 so very similar means will be calculated. Because there are more points that deviate relatively further from this mean in observation A than in Observation B, the standard deviation will reflect that

7c. What is the statistical principal that describes this phenomenon (2 pts)?
The principal that describes this is called the central limit theorem. If randomly sampling from a distribution, the means of those samples will be normally distributed. 

# Part II

Consider the four datasets, each with two columns (x and y), provided below. Be sure to replace the `NA` with your answer for each part (e.g. assign the mean of `x` for `data1` to the `data1.x.mean` variable). When you Knit your answer document, a table will be generated with all the answers.

```{r}
options(digits=2)
data1 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(8.04,6.95,7.58,8.81,8.33,9.96,7.24,4.26,10.84,4.82,5.68))
data2 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(9.14,8.14,8.74,8.77,9.26,8.1,6.13,3.1,9.13,7.26,4.74))
data3 <- data.frame(x=c(10,8,13,9,11,14,6,4,12,7,5),
					y=c(7.46,6.77,12.74,7.11,7.81,8.84,6.08,5.39,8.15,6.42,5.73))
data4 <- data.frame(x=c(8,8,8,8,8,8,8,19,8,8,8),
					y=c(6.58,5.76,7.71,8.84,8.47,7.04,5.25,12.5,5.56,7.91,6.89))

```

For each column, calculate (to two decimal places):

#### a. The mean (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.mean <- mean(data1$x) %>% round(digits = 2)
data1.y.mean <- mean(data1$y) %>% round(digits = 2)
data2.x.mean <- mean(data2$x) %>% round(digits = 2)
data2.y.mean <- mean(data2$y) %>% round(digits = 2)
data3.x.mean <- mean(data3$x) %>% round(digits = 2)
data3.y.mean <- mean(data3$y) %>% round(digits = 2)
data4.x.mean <- mean(data4$x) %>% round(digits = 2)
data4.y.mean <- mean(data4$y) %>% round(digits = 2)

```

#### b. The median (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.median <- median(data1$x) %>% round(digits = 2)
data1.y.median <- median(data1$y) %>% round(digits = 2)
data2.x.median <- median(data2$x) %>% round(digits = 2)
data2.y.median <- median(data2$y) %>% round(digits = 2)
data3.x.median <- median(data3$x) %>% round(digits = 2)
data3.y.median <- median(data3$y) %>% round(digits = 2)
data4.x.median <- median(data4$x) %>% round(digits = 2)
data4.y.median <- median(data4$y) %>% round(digits = 2)
```

#### c. The standard deviation (for x and y separately; 1 pt).

```{r include=TRUE}
data1.x.sd <- sd(data1$x) %>% round(digits = 2)
data1.y.sd <- sd(data1$y) %>% round(digits = 2)
data2.x.sd <- sd(data2$x) %>% round(digits = 2)
data2.y.sd <- sd(data2$y) %>% round(digits = 2)
data3.x.sd <- sd(data3$x) %>% round(digits = 2)
data3.y.sd <- sd(data3$y) %>% round(digits = 2)
data4.x.sd <- sd(data4$x) %>% round(digits = 2)
data4.y.sd <- sd(data4$y) %>% round(digits = 2)
```

#### For each x and y pair, calculate (also to two decimal places; 1 pt):

#### d. The correlation (1 pt).

```{r include=TRUE}

data1.correlation <- cor(data1$x,data1$y, method = "pearson") %>% round(digits = 2)
data2.correlation <- cor(data2$x,data2$y, method = "pearson") %>% round(digits = 2)
data3.correlation <- cor(data3$x,data3$y, method = "pearson") %>% round(digits = 2)
data4.correlation <- cor(data4$x,data4$y, method = "pearson") %>% round(digits = 2)
```

#### e. Linear regression equation (2 pts).

```{r include=TRUE}
data1.slope <- lm(data = data1,formula =  y~x)$coefficients[[2]]
data2.slope <- lm(data = data2,formula =  y~x)$coefficients[[2]]
data3.slope <- lm(data = data3,formula =  y~x)$coefficients[[2]]
data4.slope <- lm(data = data4,formula =  y~x)$coefficients[[2]]

data1.intercept <- lm(data = data1,formula =  y~x)$coefficients[[1]]
data2.intercept <- lm(data = data2,formula =  y~x)$coefficients[[1]]
data3.intercept <- lm(data = data3,formula =  y~x)$coefficients[[1]]
data4.intercept <- lm(data = data4,formula =  y~x)$coefficients[[1]]
```

#### f. R-Squared (2 pts).

```{r include=TRUE}
data1.rsquared <- summary(lm(data = data1,formula =  y~x))$r.squared
data2.rsquared <- summary(lm(data = data2,formula =  y~x))$r.squared
data3.rsquared <- summary(lm(data = data3,formula =  y~x))$r.squared
data4.rsquared <- summary(lm(data = data4,formula =  y~x))$r.squared
```

```{r, echo=FALSE, results='asis'}
##### DO NOT MODIFY THIS R BLOCK! ######
# This R block will create a table to display all the calculations above in one table.
library(knitr)
library(kableExtra)
results <- data.frame(
	data1.x = c(data1.x.mean, data1.x.median, data1.x.sd, data1.correlation, data1.intercept, data1.slope, data1.rsquared),
	data1.y = c(data1.y.mean, data1.y.median, data1.y.sd, NA, NA, NA, NA),
	data2.x = c(data2.x.mean, data2.x.median, data2.x.sd, data2.correlation, data2.intercept, data2.slope, data2.rsquared),
	data2.y = c(data2.y.mean, data2.y.median, data2.y.sd, NA, NA, NA, NA),
	data3.x = c(data3.x.mean, data3.x.median, data3.x.sd, data3.correlation, data3.intercept, data3.slope, data3.rsquared),
	data3.y = c(data3.y.mean, data3.y.median, data3.y.sd, NA, NA, NA, NA),
	data4.x = c(data4.x.mean, data4.x.median, data4.x.sd, data4.correlation, data4.intercept, data4.slope, data4.rsquared),
	data4.y = c(data4.y.mean, data4.y.median, data4.y.sd, NA, NA, NA, NA),
	stringsAsFactors = FALSE
)
row.names(results) <- c('Mean', 'Median', 'SD', 'r', 'Intercept', 'Slope', 'R-Squared')
names(results) <- c('x','y','x','y','x','y','x','y')
options(knitr.kable.NA = '')
kable(results, digits = 2, 
	  align = 'r',
	  row.names = TRUE, 
	  format.args=list(nsmall = 2)) %>%
	column_spec(2:9, width = ".35in") %>%
	add_header_above(c(" " = 1, "Data 1" = 2, "Data 2" = 2, "Data 3" = 2, "Data 4" = 2))
```

#### g. For each pair, is it appropriate to estimate a linear regression model? Why or why not? Be specific as to why for each pair and include appropriate plots! (4 pts)

```{r}
par(mfrow=c(2,2))
plot(data1$x,data1$y)
plot(lm(data = data1, formula = y~x), which = c(1,2))

paste0("It is appropriate to use our linear model for dataset 1 because our x/y shows a linear trend in the first plot, the residuals are randomly scttered around our model fit as shown in second plot, and our normal Q-Q plot shows normality between our two datasets in our third plot")

par(mfrow=c(2,2))
plot(data2$x,data2$y)
plot(lm(data = data2, formula = y~x), which = c(1,2))


paste0("It is not as appropriate to use our linear model for dataset 2 because our x/y shows a linear trend in the beginning but begins to take a polynomial-like function as shown in the first plot, the residuals are not randomly scttered around our model fit as shown in second plot, but our normal Q-Q plot does shows normality between our two datasets in our third plot except along the very edges")


par(mfrow=c(2,2))
plot(data3$x,data3$y)
plot(lm(data = data3, formula = y~x), which = c(1,2))

paste0("It is appropriate to use our linear model for dataset 3 because our x/y shows a linear trend in the first plot however there is a clear outlier that should be removed that is heavily skewing the data, because of that heavy skewness, our residuals arent properly scattered around our fit, and our normal Q-Q plot shows normality between our two datasets in our third plot with the exception of the outlier. The linear model would be appropriate after treating for the outlier")


par(mfrow=c(2,2))
plot(data4$x,data4$y)
plot(lm(data = data4, formula = y~x), which = c(1,2))

paste0("It is not appropriate to use our linear model for dataset 4 because our x/y shows does not show any linear trend. similar to 3, there is an outlier in our x but there does not seem to be any variation within our x dataset to define a linear function appropriately. The residuals scatter reflects this innappropriate trend in second plot")

```


#### h. Explain why it is important to include appropriate visualizations when analyzing data. Include any visualization(s) you create. (2 pts)


It is important to include appropriate visualisations when analyzing data to help understand the context in which an analysis is performed. for example the 4 datasets above all have similar statistics in terms of mean, median, etc. however, the visuals allow you to understand the distribution in better context and help justify using certain models or employing various data transformation/cleansing techniques. Visuals also allow people to quickly notice things that may otherwise not be clear such as outliers.