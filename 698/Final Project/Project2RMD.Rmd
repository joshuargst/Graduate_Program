---
title: "Project2"
author: "Joshua Registe"
date: "5/14/2021"
output:
  rmdformats::readthedown:
    self_contained: yes
    thumbnails: yes
    lightbox: yes
    gallery: no
    highlight: tango
    code_folding: hide
  html_document:
    df_print: paged
editor_options:
  chunk_output_type: console
---

```{r setup, echo=FALSE, cache=FALSE, message=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE, comment=NA, message=FALSE, warning=FALSE)
library(tidyverse)
library(kableExtra)
library(gridExtra)
library(readxl)
library(skimr)
library(DataExplorer)
library(janitor)
library(dplyr)
library(conflicted)
library(tidymodels)
library(forecast)


defaulttheme<-theme(panel.background = element_blank(),
                            panel.border = element_rect(color = "black", fill=NA))

conflict_prefer("filter","dplyr")
```

This is role playing.  I am your new boss.  I am in charge of production at ABC Beverage and you are a team of data scientists reporting to me.  My leadership has told me that new regulations are requiring us to understand our manufacturing process, the predictive factors and be able to report to them our predictive model of PH.

Please use the historical data set I am providing.  Build and report the factors in BOTH a technical and non-technical report.  I like to use Word and Excel.  Please provide your non-technical report in a  business friendly readable document and your predictions in an Excel readable format.   The technical report should show clearly the models you tested and how you selected your final approach.

Please submit both Rpubs links and .rmd files or other readable formats for technical and non-technical reports.  Also submit the excel file showing the prediction of your models for pH.


## Data Exploration
```{r}
training<-read_xlsx("StudentData.xlsx")

testing<-read_xlsx("StudentEvaluation.xlsx") %>% 
  mutate(PH = as.numeric(PH))
```

An initial glance at this dataset shows that our training set consists of a dimension of 2571 x 33 (rows x columns). Most of the features within this dataset are of a numeric nature with just 1 feature `Brand Code` being a categorical variable.

```{r}
skim(training)
```

The figure below shows the distribution of the brand codes associated with the dataset, where we observe many a skewness towards brand `B` and a few unknown data points.


```{r}
plot_bar(training)
```

In terms of numeric data, we can plot the histogram below with the `DataExplorer` package and assess the distributions of all of our features including pH. The distribution of these features do differ. pH follows a gaussian distribution which is beneficial for predictions. Many features have slight skewness or bimodal gaussian distributions. We will handle these features via boxcox transformations.


```{r, fig.height=12}
plot_histogram(training,ncol  = 3)

```


Additionally, we want to look at multicolinearity associated with the dataset. because the number of columns is quite large, we will only produce a scatter matrix using features with the highest pearson (linear) correlations. These relationships,you can see there are some strong relationships such as between filler speed vs MFR, and Balling vs Balling level, these features will need to be assessed and potentially aggregated or removed to mitigate noise in the dataset.

```{r}

trainingnumeric<-training %>% 
  select(-`Brand Code`)

ADHD_cors<-
  bind_rows(
    #pearson correlation of numeric features
    trainingnumeric %>% 
      cor(method = "pearson", use = "pairwise.complete.obs") %>% as.data.frame() %>% 
      rownames_to_column(var = "x") %>% 
      pivot_longer(cols = -x, names_to = "y", values_to = "correlation") %>% 
      mutate(cor_type = "pearson"),
    #spearman (monotonic) correlations of numeric features
    trainingnumeric %>% 
      cor(method = "spearman", use = "pairwise.complete.obs") %>% as.data.frame() %>% 
      rownames_to_column(var = "x") %>% 
      pivot_longer(cols = -x, names_to = "y", values_to = "correlation") %>% 
      mutate(cor_type = "spearman")
  )

topcors<-ADHD_cors %>% 
  filter(!(x ==y)) %>% 
  filter(cor_type=="pearson") %>% 
  #distinct(correlation,.keep_all = T) %>% 
  arrange(-abs(correlation))%>% #top_n(10, correlation) %>% 
  head(10)
  

topcorvars<-c(pull(topcors,x),pull(topcors,y)) %>% unique


```


```{r, fig.height=10, fig.width=12}

topcorset<-training %>% 
  select(all_of(c(topcorvars,'Brand Code')))
GGally::ggpairs(topcorset, progress = F, aes(color = `Brand Code`), title = "Top Correlations Within Training Set")+ theme_bw()
```


Since pH will be our main variable of interest, it is a good idea to observe any strong relationships between our predictor variables to pH, this time we will look at top spearman relationships to pH only. spearman provides a correlational strength of monotonicity which can capture both linear and non-linear relationships. We notice based on the figure below that most of the information seems a bit noisy, thus using a predictive machine learning model may provide value in identifying relationships.


```{r}
topcorsPH<-ADHD_cors %>% 
  filter(!(x ==y)) %>% 
  filter(cor_type=="spearman") %>% 
  #distinct(correlation,.keep_all = T) %>% 
  arrange(-abs(correlation))%>%
  filter(x == "PH" | y == "PH") %>% 
  head(10)

topcorvarsPH<-c(pull(topcorsPH,x),pull(topcorsPH,y)) %>% unique
```

```{r, fig.height=10, fig.width=12}

topcorsetPH<-training %>% 
  select(all_of(c(topcorvarsPH,'Brand Code')))
GGally::ggpairs(topcorsetPH, progress = F, aes(color = `Brand Code`), title = "Top Correlations to pH Within Training Set")+ theme_bw()

```


## Data Transformation



```{r}
datatrans<-training %>% recipe(~.) %>% 
  step_mutate_at(contains(c("hyd pressure", "mnf flow","filler speed","carb flow", "balling")), fn = ~ifelse(.<0,NA,.)) %>% 
  step_mutate_at(`Carb Volume`,`Filler Level`,PSC, `Air Pressurer`, `Bowl Setpoint`,`Oxygen Filler`,Temperature,MFR, fn = ~BoxCox(., lambda = BoxCox.lambda(.))) %>%
  step_mutate_at(`Brand Code`, fn = ~as.factor(.)) %>% 
  step_impute_knn(all_predictors()) %>% 
  step_rename(BrandCode=`Brand Code`) %>% 
  step_dummy(all_nominal()) %>% 
  step_corr(all_predictors(),threshold = 0.9) %>% 
  step_nzv(all_predictors()) %>% 
  prep()



training_cleaned<-
datatrans %>% bake(training)

```



For the Data preparation following baseline steps were made and the reasons are provided below

-   **Remove any NA values associated with pH**: Because `pH` will be our response variable in this project, we will omit any observations that does not have pH data (if any).


-   **Removing Negative Values**: Removing negative values associated with features where negative values dont make sense such as `Carb Volume`, `Filler Level`, `PSC`, `Air Pressurer`, `Bowl Setpoint`, and `Oxygen Filler`. 

-   **BoxCox Transformations**: Features need to be normalized such that the distributions follow gaussian curves  they are centered and scaled the mean is 0 and the Stdev is 1, this scales all the data to allow kmeans to appropriately place centroids and observations at appropriate distances.


-   **Imputation of missing data with KNN:** the remaining data was imputed with K-nearestneighbors (KNN) as a way to fill in missing gaps. alternative methods include median, mean, or bag imputations but it was determined that KNN provides the best results with minimal effect on computational effort.


-   **Dummifying Variables**: Categorical variables were binarized into 0/1. This is particularly important for certain algorithms such as linear regression that are unable to understand categorical variables. For example a feature with categories 1,2,3 will binarizing such that for example 3 would be its own column with 0/1 for presence/absence.


-   **Colinearity test**: Colinearity was tested and it was determined that there were a few features that were removed due to colinearity. the threshold used was 0.9 on a pearson-based relationship. Below shows the features that were removed due to colinearity

```{r}
datatrans$steps[[7]]$removals

```


-   **Removed low-variance features**: Removing any extremely low-variance data that will not provide useful data to the model and will only contribute to noise. At first glance, From `Data Exploration` section there doesnt appear to be any clear indicators of a low-variance variable with majority of categories recorded at a particular value. This is confirmed statistically with `tidymodels` where no features were removed due to low variance as shown below

```{r}
datatrans$steps[[8]]$removals

```


```{r, include=FALSE}
testing_cleaned<-
  datatrans %>% bake(testing)

studentdata_trans<-training_cleaned
studenteval_trans<-testing_cleaned
save(studentdata_trans,studenteval_trans,file = "data_transformed.RData")
```

